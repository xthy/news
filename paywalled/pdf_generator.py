from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime
import time
import base64
import os
import pandas as pd
from PyPDF2 import PdfMerger
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders
import re
from playwright.sync_api import sync_playwright
import tempfile
import json

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°œì„ ëœ Readability ì„¤ì • ë° ìŠ¤íƒ€ì¼
READABILITY_JS_URL = 'https://unpkg.com/@mozilla/readability@0.4.4/Readability.js'

# í•œêµ­ì–´ ìµœì í™”ëœ CSS ìŠ¤íƒ€ì¼
KOREAN_PDF_STYLE = """
<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap');
    
    * {
        box-sizing: border-box;
    }
    
    body {
        font-family: 'Noto Sans KR', 'Malgun Gothic', 'ë§‘ì€ ê³ ë”•', 'Apple SD Gothic Neo', sans-serif;
        line-height: 1.8;
        padding: 40px 30px;
        max-width: 800px;
        margin: 0 auto;
        color: #333;
        background: #fff;
        font-size: 15px;
        word-break: keep-all;
        word-wrap: break-word;
    }
    
    .article-header {
        border-bottom: 3px solid #007bff;
        margin-bottom: 30px;
        padding-bottom: 20px;
    }
    
    .article-title {
        font-size: 28px;
        font-weight: 700;
        margin-bottom: 15px;
        color: #1a1a1a;
        line-height: 1.4;
    }
    
    .article-meta {
        display: flex;
        justify-content: space-between;
        align-items: center;
        font-size: 13px;
        color: #666;
        margin-bottom: 10px;
    }
    
    .article-source {
        font-weight: 500;
        color: #007bff;
        background: #f8f9fa;
        padding: 5px 12px;
        border-radius: 15px;
        border: 1px solid #e9ecef;
    }
    
    .article-date {
        color: #999;
    }
    
    .article-byline {
        font-size: 13px;
        color: #666;
        margin-bottom: 10px;
    }
    
    .article-url {
        font-size: 11px;
        color: #999;
        word-break: break-all;
        margin-top: 5px;
    }
    
    .article-content {
        margin-top: 30px;
    }
    
    .article-content h1, .article-content h2, .article-content h3 {
        font-weight: 600;
        margin-top: 30px;
        margin-bottom: 15px;
        color: #2c3e50;
    }
    
    .article-content h1 { font-size: 24px; }
    .article-content h2 { font-size: 20px; }
    .article-content h3 { font-size: 18px; }
    
    .article-content p {
        margin-bottom: 18px;
        line-height: 1.8;
        text-align: justify;
    }
    
    .article-content img {
        max-width: 100%;
        max-height: 25vh; /* í™”ë©´ ë†’ì´ì˜ 25% ì œí•œ */
        height: auto;
        width: auto;
        margin: 15px auto;
        display: block;
        border-radius: 8px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        object-fit: contain; /* ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ í¬ê¸° ì¡°ì • */
    }
    
    /* ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ì„ ë•ŒëŠ” ìµœì†Œ í¬ê¸° ë³´ì¥ */
    .article-content img[width], .article-content img[height] {
        max-width: min(100%, 400px) !important;
        max-height: min(25vh, 300px) !important;
        width: auto !important;
        height: auto !important;
    }
    
    /* ì¸ë¼ì¸ ìŠ¤íƒ€ì¼ë¡œ ì§€ì •ëœ í° ì´ë¯¸ì§€ ê°•ì œ ì¡°ì • */
    .article-content img[style*="width"], .article-content img[style*="height"] {
        max-width: min(100%, 400px) !important;
        max-height: min(25vh, 300px) !important;
        width: auto !important;
        height: auto !important;
    }
    
    .article-content blockquote {
        border-left: 4px solid #007bff;
        padding-left: 20px;
        margin: 20px 0;
        color: #555;
        font-style: italic;
        background: #f8f9fa;
        padding: 15px 20px;
        border-radius: 0 8px 8px 0;
    }
    
    .article-content ul, .article-content ol {
        margin: 15px 0;
        padding-left: 25px;
    }
    
    .article-content li {
        margin-bottom: 8px;
        line-height: 1.6;
    }
    
    .copyright-info {
        margin-top: 40px;
        padding-top: 20px;
        border-top: 1px solid #eee;
        font-size: 12px;
        color: #999;
        text-align: center;
    }
    
    @media print {
        body {
            padding: 20px;
            font-size: 14px;
        }
        .article-title {
            font-size: 24px;
        }
        /* ì¸ì‡„ ì‹œ ì´ë¯¸ì§€ í¬ê¸° ë” ì—„ê²©í•˜ê²Œ ì œí•œ */
        .article-content img {
            max-width: 80% !important;
            max-height: 20vh !important;
            page-break-inside: avoid;
        }
    }
</style>
"""

# ì–¸ë¡ ì‚¬ ì‹ë³„ì„ ìœ„í•œ íŒ¨í„´ ë§¤í•‘
MEDIA_PATTERNS = {
    # ì–¸ë¡ ì‚¬ë³„ ë„ë©”ì¸ê³¼ ì´ë¦„ ë§¤í•‘
    'domain_mapping': {
        'thebell.co.kr': 'ë”ë²¨',
        'investchosun.com': 'ì¸ë² ìŠ¤íŠ¸ì¡°ì„ ',
        'marketinsight.hankyung.com': 'í•œê²½ë§ˆì¼“ì¸ì‚¬ì´íŠ¸',
        'news.naver.com': 'ë„¤ì´ë²„ë‰´ìŠ¤',
        'chosun.com': 'ì¡°ì„ ì¼ë³´',
        'joongang.co.kr': 'ì¤‘ì•™ì¼ë³´',
        'donga.com': 'ë™ì•„ì¼ë³´',
        'hankyung.com': 'í•œêµ­ê²½ì œ',
        'mk.co.kr': 'ë§¤ì¼ê²½ì œ',
        'edaily.co.kr': 'ì´ë°ì¼ë¦¬',
        'newsway.co.kr': 'ë‰´ìŠ¤ì›¨ì´',
        'newspim.com': 'ë‰´ìŠ¤í•Œ',
        'ajunews.com': 'ì•„ì£¼ê²½ì œ',
        'etnews.com': 'ì „ìì‹ ë¬¸',
        'dt.co.kr': 'ë””ì§€í„¸íƒ€ì„ìŠ¤',
        'zdnet.co.kr': 'ZDNet Korea',
        'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',
        'yna.co.kr': 'ì—°í•©ë‰´ìŠ¤'
    }
}

# ê°œì„ ëœ JS ìŠ¤í¬ë¦½íŠ¸
def get_enhanced_js_script():
    domain_mapping_json = json.dumps(MEDIA_PATTERNS['domain_mapping'])
    
    return f"""
(async function() {{
    try {{
        // Readability ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
        const response = await fetch("{READABILITY_JS_URL}");
        const readabilityCode = await response.text();
        eval(readabilityCode);
        
        // ê¸°ì‚¬ ì¶”ì¶œ
        const documentClone = document.cloneNode(true);
        const reader = new Readability(documentClone);
        const article = reader.parse();
        
        if (!article) {{
            return null;
        }}
        
        // ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ì¶œ
        function extractMediaInfo() {{
            const url = window.location.href;
            let mediaName = '';
            let copyrightInfo = '';
            
            // 1. ë„ë©”ì¸ ê¸°ë°˜ ì–¸ë¡ ì‚¬ ì‹ë³„
            const domain = window.location.hostname;
            const domainMapping = {domain_mapping_json};
            
            for (const [domainPattern, name] of Object.entries(domainMapping)) {{
                if (domain.includes(domainPattern)) {{
                    mediaName = name;
                    break;
                }}
            }}
            
            // 2. ì €ì‘ê¶Œ ì •ë³´ ì°¾ê¸°
            const bodyText = document.body.innerText;
            const copyrightPatterns = [
                /Â©\\s*([^,\\n]+)/g,
                /ì €ì‘ê¶Œì\\s*Â©\\s*([^,\\n]+)/g,
                /Copyright\\s*Â©\\s*([^,\\n]+)/g,
                /â“’\\s*([^,\\n]+)/g,
                /Â©\\s*(\\d{{4}})\\s*([^,\\n]+)/g
            ];
            
            for (const pattern of copyrightPatterns) {{
                const matches = bodyText.match(pattern);
                if (matches && matches.length > 0) {{
                    copyrightInfo = matches[0].trim();
                    // ì–¸ë¡ ì‚¬ ì´ë¦„ì´ ì €ì‘ê¶Œ ì •ë³´ì— ìˆìœ¼ë©´ ì¶”ì¶œ
                    if (!mediaName && copyrightInfo) {{
                        const cleanCopyright = copyrightInfo.replace(/Â©|ì €ì‘ê¶Œì|Copyright|â“’/g, '').trim();
                        if (cleanCopyright) {{
                            mediaName = cleanCopyright.split(',')[0].trim();
                        }}
                    }}
                    break;
                }}
            }}
            
            // 3. ë©”íƒ€ íƒœê·¸ì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ì°¾ê¸°
            if (!mediaName) {{
                const metaTags = [
                    'og:site_name',
                    'twitter:site',
                    'article:publisher',
                    'author'
                ];
                
                for (const tag of metaTags) {{
                    const meta = document.querySelector(`meta[property="${{tag}}"], meta[name="${{tag}}"]`);
                    if (meta && meta.content) {{
                        mediaName = meta.content;
                        break;
                    }}
                }}
            }}
            
            // 4. ë„¤ì´ë²„ ë‰´ìŠ¤ì˜ ê²½ìš° ì›ë³¸ ì–¸ë¡ ì‚¬ ì°¾ê¸°
            if (domain.includes('news.naver.com')) {{
                const pressLogo = document.querySelector('.press_logo img');
                const pressName = document.querySelector('.press_logo .name');
                const mediaEndArea = document.querySelector('.media_end_head_top .media_end_head_top_logo img');
                
                if (pressName) {{
                    mediaName = pressName.textContent.trim();
                }} else if (pressLogo && pressLogo.alt) {{
                    mediaName = pressLogo.alt.trim();
                }} else if (mediaEndArea && mediaEndArea.alt) {{
                    mediaName = mediaEndArea.alt.trim();
                }} else {{
                    // ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ì°¾ê¸°
                    const articleByline = document.querySelector('.article_byline, .byline');
                    if (articleByline) {{
                        const bylineText = articleByline.textContent;
                        const mediaMatch = bylineText.match(/([ê°€-í£]+)\\s*ê¸°ì/);
                        if (mediaMatch) {{
                            mediaName = mediaMatch[1];
                        }}
                    }}
                }}
            }}
            
            return {{
                mediaName: mediaName || 'ì•Œ ìˆ˜ ì—†ëŠ” ì–¸ë¡ ì‚¬',
                copyrightInfo: copyrightInfo,
                url: url
            }};
        }}
        
        const mediaInfo = extractMediaInfo();
        
        // ë°œí–‰ì¼ ì¶”ì¶œ ê°œì„ 
        function extractPublishDate() {{
            // ë©”íƒ€ íƒœê·¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
            const dateSelectors = [
                'meta[property="article:published_time"]',
                'meta[property="article:modified_time"]',
                'meta[name="pubdate"]',
                'meta[name="date"]',
                'time[datetime]',
                '.date', '.publish-date', '.article-date'
            ];
            
            for (const selector of dateSelectors) {{
                const element = document.querySelector(selector);
                if (element) {{
                    const dateValue = element.getAttribute('content') || 
                                    element.getAttribute('datetime') || 
                                    element.textContent;
                    if (dateValue) {{
                        try {{
                            const date = new Date(dateValue);
                            if (!isNaN(date.getTime())) {{
                                return date.toLocaleDateString('ko-KR', {{
                                    year: 'numeric',
                                    month: 'long',
                                    day: 'numeric',
                                    hour: '2-digit',
                                    minute: '2-digit'
                                }});
                            }}
                        }} catch (e) {{
                            // ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
                        }}
                    }}
                }}
            }}
            
            return new Date().toLocaleDateString('ko-KR', {{
                year: 'numeric',
                month: 'long',
                day: 'numeric'
            }});
        }}
        
        const publishDate = extractPublishDate();
        
        return {{
            title: article.title,
            content: article.content,
            textContent: article.textContent,
            length: article.length,
            excerpt: article.excerpt,
            byline: article.byline,
            mediaName: mediaInfo.mediaName,
            copyrightInfo: mediaInfo.copyrightInfo,
            publishDate: publishDate,
            url: mediaInfo.url
        }};
        
    }} catch (error) {{
        console.error('Enhanced readability extraction failed:', error);
        return null;
    }}
}})();
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í‚¤ì›Œë“œ ê·¸ë£¹í•‘ ë§¤í•‘
KEYWORD_GROUPING = {
    # ì˜ë¬¸ ë§¤í•‘ (ê¸°ì¡´)
    'êµë³´ìƒëª…': 'Kyobo Life',
    'ë²„ê±°í‚¹': 'BKR',
    'íŒ€í™€íŠ¼': 'BKR',
    'ë§¥ë„ë‚ ë“œ': 'BKR',
    'í˜„ëŒ€ì»¤ë¨¸ì…œ': 'HCI',
    'ìœ ë² ì´ìŠ¤': 'UBase',
    'ì„œë¸Œì›': 'Serveone',
    'ë½ì•¤ë½': 'Lock&Lock',
    'ì¡ì½”ë¦¬ì•„': 'JOBKOREA',
    'ì•Œë°”ëª¬': 'JOBKOREA',
    'ìš”ê¸°ìš”': 'YGY',
    'ì¿ íŒ¡ì´ì¸ ': 'YGY',
    'ë°°ë‹¬ì˜ë¯¼ì¡±': 'YGY',
    'ë°°ë¯¼': 'YGY',
    'SKë Œí„°ì¹´': 'SK Rent-a-Car',
    # Market (ë¡¯ë°ë Œíƒˆ + ì–´í”¼ë‹ˆí‹° ê´€ë ¨)
    'ë¡¯ë°ë Œíƒˆ': 'Market',
    'ë¡¯ë°ë Œí„°ì¹´': 'Market',
    'ì–´í”¼ë‹ˆí‹°': 'Market',
    'ì–´í”¼ë„ˆí‹°': 'Market'
}

# ê·¸ë£¹ ìˆœì„œ ì •ì˜
GROUP_ORDER = [
    'Kyobo Life',
    'BKR',
    'HCI',
    'UBase',
    'Serveone',
    'Shinhan Financial Group',
    'Lock&Lock',
    'JOBKOREA',
    'YGY',
    'SK Rent-a-Car',
    'Market'
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì´íŠ¸ë³„ ì„¤ì •
SITE_CONFIGS = {
    "thebell": {
        "name": "ë”ë²¨",
        "login_url": "https://www.thebell.co.kr/LoginCert/Login.asp",
        "id": "affinity",
        "password": "equity",
        "login_selectors": {
            "id_field": "id",
            "pw_field": "pw",
            "login_button": "#btn1"
        }
    },
    "investchosun": {
        "name": "ì¸ë² ìŠ¤íŠ¸ì¡°ì„ ",
        "login_url": "https://www.investchosun.com/svc/member/invest_login.html",
        "id": "affini1",
        "password": "affini12026",
        "login_selectors": {
            "id_field": "username",
            "pw_field": "login_password",
            "login_button": "#SignIn"
        }
    },
    "hankyung": {
        "name": "í•œê²½ë§ˆì¼“ì¸ì‚¬ì´íŠ¸",
        "login_url": "https://marketinsight.hankyung.com/",
        "id": "affini1",
        "password": "affini1",
        "login_selectors": {
            "id_field": "user_id",
            "pw_field": "password",
            "login_button": ".btnLogin"
        }
    },
    "naver": {
        "name": "ë„¤ì´ë²„"
    }
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ë©”ì¼ ì„¤ì •
EMAIL_CONFIG = {
    "smtp_server": "smtp.gmail.com",
    "smtp_port": 587,
    "sender_email": "xxxthy@gmail.com",
    "sender_password": "ckxi cbhu myzv ejyt",
    "recipient_email": [
        "thyang@affinityequity.com",
        "xxxthy@gmail.com",
        "mistyjo12@outlook.kr",
        "nkcho@affinityequity.com",
        "smpark@affinityequity.com"
    ]
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚ ì§œ ê¸°ë°˜ íŒŒì¼ëª… ì„¤ì •
today_str = datetime.today().strftime('%Y%m%d')
# ë©”ì¼ìš© ë‚ ì§œ í˜•ì‹ (19Jun25)
today_date_obj = datetime.today()
mail_date_str = today_date_obj.strftime('%d%b%y')  # 19Jun25
excel_filename = f"curated_list_{today_str}.xlsx"
pdf_output_dir = f"news_articles_pdf_{today_str}"
merged_pdf_filename = f"News run_{mail_date_str}.pdf"

# Chrome ì„¤ì •
chrome_path = "C:/Program Files/Google/Chrome/Application/chrome.exe"
chromedriver_path = "C:/Users/Bloomberg/AppData/Local/Microsoft/WindowsApps/chromedriver.exe"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì´íŠ¸ ê°ì§€ í•¨ìˆ˜
def detect_site_from_url(url):
    """URLì„ ë³´ê³  ì–´ëŠ ì‚¬ì´íŠ¸ì¸ì§€ íŒë‹¨"""
    if "thebell.co.kr" in url:
        return "thebell"
    elif "investchosun.com" in url:
        return "investchosun"
    elif "marketinsight.hankyung.com" in url:
        return "hankyung"
    else:
        return "naver"  # ê·¸ ì™¸ ëª¨ë“  URLì€ ë„¤ì´ë²„ë¡œ ì²˜ë¦¬

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°œì„ ëœ Readability í•¨ìˆ˜ë“¤
def extract_article_with_enhanced_playwright(url):
    """ê°œì„ ëœ Playwrightì™€ Readabilityë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ì¶”ì¶œ"""
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            # í•œêµ­ì–´ í°íŠ¸ ì§€ì›ì„ ìœ„í•œ ì„¤ì •
            page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            })
            
            page.goto(url, timeout=60000, wait_until='networkidle')
            page.wait_for_timeout(3000)
            
            # ê°œì„ ëœ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            article = page.evaluate(get_enhanced_js_script())
            
            # ì´ë¯¸ì§€ í¬ê¸° ì œí•œì„ ìœ„í•œ ì¶”ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            image_resize_script = """
            // ëª¨ë“  ì´ë¯¸ì§€ì— í¬ê¸° ì œí•œ ì ìš©
            const images = document.querySelectorAll('img');
            images.forEach(img => {
                // ì¸ë¼ì¸ ìŠ¤íƒ€ì¼ ì œê±°
                img.removeAttribute('style');
                img.removeAttribute('width');
                img.removeAttribute('height');
                
                // CSS í´ë˜ìŠ¤ ì¶”ê°€ë¡œ í¬ê¸° ì œí•œ
                img.style.cssText = `
                    max-width: min(100%, 400px) !important;
                    max-height: min(25vh, 300px) !important;
                    width: auto !important;
                    height: auto !important;
                    object-fit: contain !important;
                    display: block !important;
                    margin: 15px auto !important;
                `;
            });
            """
            page.evaluate(image_resize_script)
            
            browser.close()
            
            if article and article.get('title') and article.get('content'):
                # í˜„ì¬ ë‚ ì§œ ì •ë³´
                current_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
                
                # HTML í…œí”Œë¦¿ ìƒì„±
                html_content = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{article['title']}</title>
    {KOREAN_PDF_STYLE}
</head>
<body>
    <div class="article-header">
        <h1 class="article-title">{article['title']}</h1>
        <div class="article-meta">
            <span class="article-source">{article.get('mediaName', 'ì•Œ ìˆ˜ ì—†ëŠ” ì–¸ë¡ ì‚¬')}</span>
            <span class="article-date">{article.get('publishDate', current_date)}</span>
        </div>
        {f'<div class="article-byline">ê¸°ì: {article["byline"]}</div>' if article.get('byline') else ''}
        <div class="article-url">{article.get('url', url)}</div>
    </div>
    
    <div class="article-content">
        {article['content']}
    </div>
    
    <div class="copyright-info">
        {f'<div>{article["copyrightInfo"]}</div>' if article.get('copyrightInfo') else ''}
        <div>ì¶œì²˜: {article.get('mediaName', 'ì•Œ ìˆ˜ ì—†ëŠ” ì–¸ë¡ ì‚¬')}</div>
        <div>ì›ë¬¸ ë§í¬: {article.get('url', url)}</div>
    </div>
</body>
</html>
"""
                return html_content
            else:
                return None
                
    except Exception as e:
        print(f"   âš ï¸ Enhanced Readability ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

def create_enhanced_pdf_from_html(html_content):
    """ê°œì„ ëœ HTMLì„ PDFë¡œ ë³€í™˜"""
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            # í•œêµ­ì–´ í°íŠ¸ë¥¼ ìœ„í•œ ì„¤ì •
            page.set_content(html_content, wait_until='networkidle')
            
            # PDF ìƒì„± ì˜µì…˜ ê°œì„ 
            pdf_data = page.pdf(
                format='A4',
                print_background=True,
                margin={
                    'top': '20mm',
                    'bottom': '20mm', 
                    'left': '15mm',
                    'right': '15mm'
                },
                prefer_css_page_size=True
            )
            
            browser.close()
            return pdf_data
            
    except Exception as e:
        print(f"   âš ï¸ PDF ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def process_naver_article_enhanced(origin_url, idx, total_articles):
    """ê°œì„ ëœ ë„¤ì´ë²„ ê¸°ì‚¬ ì²˜ë¦¬"""
    try:
        print(f"   ğŸ”„ Readabilityë¡œ ê¸°ì‚¬ ì¶”ì¶œ ì¤‘...")
        html_content = extract_article_with_enhanced_playwright(origin_url)
        
        if html_content:
            print(f"   ğŸ“„ PDF ë³€í™˜ ì¤‘...")
            pdf_data = create_enhanced_pdf_from_html(html_content)
            
            if pdf_data:
                print(f"   âœ… {idx+1}/{total_articles}: ë„¤ì´ë²„ ê¸°ì‚¬ PDF ìƒì„± ì™„ë£Œ")
                return pdf_data
            else:
                raise Exception("PDF ë³€í™˜ ì‹¤íŒ¨")
        else:
            raise Exception("Readabilityë¡œ ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"   âš ï¸ {idx+1}/{total_articles}: ë„¤ì´ë²„ PDF ìƒì„± ì‹¤íŒ¨ - {e}")
        return None

def enhance_pdf_for_other_sites(driver, print_url):
    """ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë“¤ì˜ PDF í’ˆì§ˆ ê°œì„ """
    try:
        # í˜ì´ì§€ ë¡œë”© í›„ í•œêµ­ì–´ í°íŠ¸ ìŠ¤íƒ€ì¼ ì£¼ì…
        driver.get(print_url)
        time.sleep(3)
        
        # í•œêµ­ì–´ ìµœì í™” CSS ì£¼ì…
        korean_style_script = """
        const style = document.createElement('style');
        style.textContent = `
            body {
                font-family: 'Noto Sans KR', 'Malgun Gothic', 'ë§‘ì€ ê³ ë”•', 'Apple SD Gothic Neo', sans-serif !important;
                line-height: 1.8 !important;
                word-break: keep-all !important;
                word-wrap: break-word !important;
            }
            * {
                font-family: 'Noto Sans KR', 'Malgun Gothic', 'ë§‘ì€ ê³ ë”•', 'Apple SD Gothic Neo', sans-serif !important;
            }
            h1, h2, h3, h4, h5, h6 {
                font-weight: 600 !important;
                margin-top: 20px !important;
                margin-bottom: 10px !important;
            }
            p {
                margin-bottom: 15px !important;
                text-align: justify !important;
            }
            img {
                max-width: 100% !important;
                max-height: min(25vh, 300px) !important;
                height: auto !important;
                width: auto !important;
                margin: 15px auto !important;
                display: block !important;
                object-fit: contain !important;
            }
        `;
        document.head.appendChild(style);
        """
        
        driver.execute_script(korean_style_script)
        time.sleep(1)  # ìŠ¤íƒ€ì¼ ì ìš© ëŒ€ê¸°
        
        # ê°œì„ ëœ PDF ìƒì„± ì˜µì…˜
        result = driver.execute_cdp_cmd("Page.printToPDF", {
            "printBackground": True,
            "preferCSSPageSize": True,
            "paperWidth": 8.27,  # A4 width in inches
            "paperHeight": 11.69,  # A4 height in inches
            "marginTop": 0.8,
            "marginBottom": 0.8,
            "marginLeft": 0.6,
            "marginRight": 0.6,
            "displayHeaderFooter": False,
            "scale": 0.9  # ì•½ê°„ ì¶•ì†Œí•˜ì—¬ ë” ë§ì€ ë‚´ìš©ì´ ë“¤ì–´ê°€ë„ë¡
        })
        
        return base64.b64decode(result['data'])
        
    except Exception as e:
        print(f"   âš ï¸ PDF í’ˆì§ˆ ê°œì„  ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
        # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°±
        result = driver.execute_cdp_cmd("Page.printToPDF", {
            "printBackground": True,
            "preferCSSPageSize": True
        })
        return base64.b64decode(result['data'])

# ê¸°ì¡´ extract_article_with_playwright í•¨ìˆ˜ë¥¼ ëŒ€ì²´
def extract_article_with_playwright(url):
    """ê°œì„ ëœ ê¸°ì‚¬ ì¶”ì¶œ í•¨ìˆ˜ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    return extract_article_with_enhanced_playwright(url)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# URL ë³€í™˜ í•¨ìˆ˜ë“¤
def convert_thebell_url(origin_url):
    """ë”ë²¨ URLì„ PDF ì¶œë ¥ìš© URLë¡œ ë³€í™˜"""
    try:
        news_key = origin_url.split("key=")[-1]
        return f"https://www.thebell.co.kr/front/NewsPrint.asp?key={news_key}"
    except:
        return None

def convert_investchosun_url(origin_url):
    """ì¸ë² ìŠ¤íŠ¸ì¡°ì„  URLì„ PDF ì¶œë ¥ìš© URLë¡œ ë³€í™˜"""
    try:
        match = re.search(r'/(\d+)\.html$', origin_url)
        if match:
            contid = match.group(1)
            return f"https://www.investchosun.com/svc/news/article_print.html?contid={contid}"
        else:
            print(f"   âš ï¸ URL íŒ¨í„´ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {origin_url}")
            return None
    except Exception as e:
        print(f"   âš ï¸ URL ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None

def convert_hankyung_url(origin_url):
    """í•œê²½ë§ˆì¼“ì¸ì‚¬ì´íŠ¸ URLì„ PDF ì¶œë ¥ìš© URLë¡œ ë³€í™˜"""
    try:
        if "/article/" in origin_url:
            print_url = origin_url.replace("/article/", "/print/")
            return print_url
        else:
            print(f"   âš ï¸ URL íŒ¨í„´ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {origin_url}")
            return None
    except Exception as e:
        print(f"   âš ï¸ URL ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None

def convert_naver_url(origin_url):
    """ë„¤ì´ë²„ URLì€ ë³€í™˜ ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (Readabilityë¡œ ì²˜ë¦¬)"""
    return origin_url

# URL ë³€í™˜ í•¨ìˆ˜ ë§¤í•‘
URL_CONVERTERS = {
    "thebell": convert_thebell_url,
    "investchosun": convert_investchosun_url,
    "hankyung": convert_hankyung_url,
    "naver": convert_naver_url
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŒŒì¼ëª… ë³€í™˜ í•¨ìˆ˜
def get_english_filename(korean_filename, today_str):
    """í•œê¸€ íŒŒì¼ëª…ì„ ì˜ë¬¸ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜"""
    # News run_19Jun25.pdf í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return merged_pdf_filename

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì´íŠ¸ë³„ ë¡œê·¸ì¸ í•¨ìˆ˜
def login_to_site(driver, site_key):
    """ì‚¬ì´íŠ¸ë³„ ë¡œê·¸ì¸ ìˆ˜í–‰"""
    config = SITE_CONFIGS[site_key]
    
    try:
        print(f"ğŸ” {config['name']} ë¡œê·¸ì¸ ì¤‘...")
        driver.get(config["login_url"])
        time.sleep(2)
        
        # ë¡œê·¸ì¸ ì •ë³´ ì…ë ¥
        id_field = driver.find_element(By.ID, config["login_selectors"]["id_field"])
        pw_field = driver.find_element(By.ID, config["login_selectors"]["pw_field"])
        
        id_field.clear()
        id_field.send_keys(config["id"])
        pw_field.clear()
        pw_field.send_keys(config["password"])
        
        # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
        login_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, config["login_selectors"]["login_button"]))
        )
        login_button.click()
        time.sleep(3)
        
        print(f"   âœ… {config['name']} ë¡œê·¸ì¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        print(f"   âŒ {config['name']} ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—‘ì…€ ë°ì´í„° ë¶„ì„ í•¨ìˆ˜
def analyze_excel_data():
    """ì—‘ì…€ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì‚¬ì´íŠ¸ë³„ ê¸°ì‚¬ ìˆ˜ë¥¼ íŒŒì•…"""
    try:
        df = pd.read_excel(excel_filename)
        
        if 'ë§í¬' not in df.columns:
            print("   âŒ 'ë§í¬' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None, {}
        
        # ì‚¬ì´íŠ¸ë³„ ê¸°ì‚¬ ë¶„ë¥˜
        site_articles = {}
        total_articles = len(df)
        
        for idx, row in df.iterrows():
            url = row['ë§í¬']
            site_key = detect_site_from_url(url)
            
            if site_key not in site_articles:
                site_articles[site_key] = []
            site_articles[site_key].append({
                'index': idx,
                'url': url,
                'title': row.get('ì œëª©', f'ê¸°ì‚¬_{idx+1}')
            })
        
        # í†µê³„ ì¶œë ¥
        print(f"   ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ë°œê²¬")
        for site_key, articles in site_articles.items():
            site_name = SITE_CONFIGS[site_key]['name']
            print(f"   - {site_name}: {len(articles)}ê°œ")
        
        return df, site_articles
        
    except Exception as e:
        print(f"   âŒ ì—‘ì…€ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None, {}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í†µí•© PDF ìƒì„± í•¨ìˆ˜
def generate_unified_pdf(driver):
    """ëª¨ë“  ì‚¬ì´íŠ¸ì˜ ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì˜ PDFë¡œ ìƒì„±"""
    print(f"\nğŸ“„ í†µí•© PDF ìƒì„± ì‹œì‘...")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(pdf_output_dir, exist_ok=True)
    
    # ì—‘ì…€ ë°ì´í„° ë¶„ì„
    df, site_articles = analyze_excel_data()
    if df is None:
        return None, 0, {}
    
    total_articles = len(df)
    if total_articles == 0:
        print("   â— ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, 0, {}
    
    # ë¡œê·¸ì¸ëœ ì‚¬ì´íŠ¸ ì¶”ì 
    logged_in_sites = set()
    pdf_paths = []
    success_count = 0
    site_stats = {}
    
    # ì‚¬ì´íŠ¸ë³„ í†µê³„ ì´ˆê¸°í™”
    for site_key in site_articles.keys():
        site_stats[site_key] = {
            'name': SITE_CONFIGS[site_key]['name'],
            'total': len(site_articles[site_key]),
            'success': 0,
            'failed': 0
        }
    
    # ê° ê¸°ì‚¬ë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
    for idx, row in df.iterrows():
        try:
            origin_url = row['ë§í¬']
            title = row.get('ì œëª©', f'ê¸°ì‚¬_{idx+1}')
            site_key = detect_site_from_url(origin_url)
            
            site_name = SITE_CONFIGS[site_key]['name']
            print(f"   ğŸ”„ {idx+1}/{total_articles}: {site_name} - {title[:50]}...")
            
            # ë„¤ì´ë²„ëŠ” ë¡œê·¸ì¸ ë¶ˆí•„ìš”, ë‹¤ë¥¸ ì‚¬ì´íŠ¸ëŠ” í•„ìš”ì‹œ ë¡œê·¸ì¸
            if site_key != "naver" and site_key not in logged_in_sites:
                if login_to_site(driver, site_key):
                    logged_in_sites.add(site_key)
                else:
                    print(f"   âŒ {site_name} ë¡œê·¸ì¸ ì‹¤íŒ¨ë¡œ ê±´ë„ˆëœ€")
                    site_stats[site_key]['failed'] += 1
                    continue
            
            # URL ë³€í™˜
            url_converter = URL_CONVERTERS[site_key]
            print_url = url_converter(origin_url)
            
            if not print_url:
                print(f"   âš ï¸ {idx+1}/{total_articles}: URL ë³€í™˜ ì‹¤íŒ¨")
                site_stats[site_key]['failed'] += 1
                continue
            
            # PDF ìƒì„±
            pdf_data = None
            
            if site_key == "naver":
                # ê°œì„ ëœ ë„¤ì´ë²„ ì²˜ë¦¬
                pdf_data = process_naver_article_enhanced(print_url, idx, total_articles)
                if not pdf_data:
                    site_stats[site_key]['failed'] += 1
                    continue
            else:
                # ê¸°ì¡´ ì‚¬ì´íŠ¸ë“¤ì€ ê°œì„ ëœ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                pdf_data = enhance_pdf_for_other_sites(driver, print_url)
            
            # íŒŒì¼ëª… ìƒì„±
            if site_key == "thebell":
                file_id = origin_url.split("key=")[-1]
            elif site_key == "investchosun":
                match = re.search(r'/(\d+)\.html$', origin_url)
                file_id = match.group(1) if match else f"article_{idx+1}"
            elif site_key == "hankyung":
                match = re.search(r'/([^/]+)$', origin_url)
                file_id = match.group(1) if match else f"article_{idx+1}"
            elif site_key == "naver":
                # ë„¤ì´ë²„ URLì—ì„œ ê³ ìœ  ID ì¶”ì¶œ
                match = re.search(r'article/(\d+)', origin_url)
                if not match:
                    match = re.search(r'aid=(\d+)', origin_url)
                file_id = match.group(1) if match else f"article_{idx+1}"
            else:
                file_id = f"article_{idx+1}"
            
            pdf_path = os.path.join(pdf_output_dir, f"{idx+1:03d}_{site_key}_{file_id}.pdf")
            with open(pdf_path, "wb") as f:
                f.write(pdf_data)
            pdf_paths.append(pdf_path)
            success_count += 1
            site_stats[site_key]['success'] += 1
            
            print(f"   âœ… {idx+1}/{total_articles}: ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"   âš ï¸ {idx+1}/{total_articles} ì‹¤íŒ¨: {e}")
            if site_key in site_stats:
                site_stats[site_key]['failed'] += 1
    
    # PDF ë³‘í•©
    if pdf_paths:
        print(f"\nğŸ”— í†µí•© PDF ë³‘í•© ì¤‘... ({len(pdf_paths)}ê°œ íŒŒì¼)")
        
        # íŒŒì¼ í¬ê¸° ê³„ì‚°
        total_size_mb = sum(os.path.getsize(path) for path in pdf_paths) / (1024 * 1024)
        print(f"   ğŸ“Š ì˜ˆìƒ í†µí•© PDF í¬ê¸°: {total_size_mb:.1f}MB")
        
        try:
            merger = PdfMerger()
            for path in pdf_paths:
                merger.append(path)
            merger.write(merged_pdf_filename)
            merger.close()
            
            final_size_mb = os.path.getsize(merged_pdf_filename) / (1024 * 1024)
            print(f"   âœ… ë³‘í•© ì™„ë£Œ: {merged_pdf_filename} ({final_size_mb:.1f}MB)")
            
            # ê°œë³„ íŒŒì¼ ì •ë¦¬
            print(f"   ğŸ—‘ï¸ ê°œë³„ PDF íŒŒì¼ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤...")
            deleted_count = 0
            for pdf_path in pdf_paths:
                try:
                    os.remove(pdf_path)
                    deleted_count += 1
                except Exception as e:
                    print(f"   ì‚­ì œ ì‹¤íŒ¨: {os.path.basename(pdf_path)} â†’ {e}")
            
            # ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ
            try:
                if os.path.exists(pdf_output_dir) and not os.listdir(pdf_output_dir):
                    os.rmdir(pdf_output_dir)
            except:
                pass
            
            print(f"   âœ… ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ ê°œë³„ íŒŒì¼ ì‚­ì œ")
            
            return merged_pdf_filename, success_count, site_stats
            
        except Exception as e:
            print(f"   âŒ PDF ë³‘í•© ì‹¤íŒ¨: {e}")
            return None, success_count, site_stats
    else:
        print(f"   â— ìƒì„±ëœ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, 0, site_stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ë©”ì¼ ë°œì†¡ í•¨ìˆ˜
def send_email_with_pdf(pdf_file, total_articles, site_stats):
    """ìƒì„±ëœ PDFë¥¼ ì´ë©”ì¼ë¡œ ë°œì†¡"""
    try:
        print("\nğŸ“§ ì´ë©”ì¼ ë°œì†¡ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # PDF íŒŒì¼ í™•ì¸
        if not pdf_file or not os.path.exists(pdf_file):
            print("   âš ï¸ ì²¨ë¶€í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì´ë©”ì¼ ë°œì†¡ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return False
        
        pdf_size_mb = os.path.getsize(pdf_file) / (1024 * 1024)
        print(f"   ğŸ“„ PDF íŒŒì¼: {pdf_file} ({pdf_size_mb:.1f}MB)")
        
        # ì—‘ì…€ íŒŒì¼ ë‹¤ì‹œ ì½ì–´ì„œ ê·¸ë£¹ë³„ ê¸°ì‚¬ ì •ë¦¬
        df = pd.read_excel(excel_filename)
        grouped_news = {}
        
        # í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”
        for idx, row in df.iterrows():
            keyword = row.get('ê²€ìƒ‰í‚¤ì›Œë“œ', row.get('ê²€ìƒ‰ í‚¤ì›Œë“œ', row.get('í‚¤ì›Œë“œ', '')))
            if pd.isna(keyword) or keyword == '':
                continue
            
            # ê·¸ë£¹ ì°¾ê¸°
            group_name = KEYWORD_GROUPING.get(keyword, None)
            if group_name:
                if group_name not in grouped_news:
                    grouped_news[group_name] = []
                
                # ê¸°ì‚¬ ì •ë³´ ì €ì¥
                title = row.get('ì œëª©', '')
                url = row.get('ë§í¬', '')
                # ì–¸ë¡ ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì—‘ì…€ì˜ 'ì–¸ë¡ ì‚¬' ì»¬ëŸ¼ ì‚¬ìš©)
                media_name = row.get('ì–¸ë¡ ì‚¬', '')
                if pd.isna(media_name) or media_name == '':
                    # ì–¸ë¡ ì‚¬ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì‚¬ì´íŠ¸ëª… ì‚¬ìš©
                    site_key = detect_site_from_url(url)
                    media_name = SITE_CONFIGS[site_key]['name']
                
                pub_date = row.get('ë‚ ì§œ', row.get('ê²Œì¬ì¼', ''))
                
                # ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬ - í•­ìƒ DDMmm í˜•ì‹ìœ¼ë¡œ
                if isinstance(pub_date, pd.Timestamp):
                    pub_date = pub_date.strftime('%d%b')
                elif isinstance(pub_date, str):
                    if len(pub_date) == 8 and pub_date.isdigit():
                        # 20250619 -> 19Jun
                        from datetime import datetime as dt
                        date_obj = dt.strptime(pub_date, '%Y%m%d')
                        pub_date = date_obj.strftime('%d%b')
                    elif '-' in pub_date:
                        # 2025-06-19 í˜•ì‹ ì²˜ë¦¬ -> 19Jun
                        try:
                            from datetime import datetime as dt
                            date_obj = dt.strptime(pub_date.split(' ')[0], '%Y-%m-%d')
                            pub_date = date_obj.strftime('%d%b')
                        except:
                            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
                            pub_date = '19Jun'
                    elif '.' in pub_date:
                        # 2025.06.18 í˜•ì‹ ì²˜ë¦¬ -> 18Jun
                        try:
                            from datetime import datetime as dt
                            date_obj = dt.strptime(pub_date.split(' ')[0], '%Y.%m.%d')
                            pub_date = date_obj.strftime('%d%b')
                        except:
                            pub_date = '19Jun'
                else:
                    pub_date = '19Jun'  # ê¸°ë³¸ê°’
                
                grouped_news[group_name].append({
                    'title': title,
                    'date': pub_date,
                    'source': media_name
                })
        
        # ì´ë©”ì¼ ë©”ì‹œì§€ ìƒì„±
        msg = MIMEMultipart()
        msg['From'] = EMAIL_CONFIG["sender_email"]
        
        # ìˆ˜ì‹ ì ì²˜ë¦¬ - BCCë¡œ ë³€ê²½
        recipients = EMAIL_CONFIG["recipient_email"]
        if isinstance(recipients, list):
            msg['To'] = ""  # To í•„ë“œëŠ” ë¹„ì›Œë‘ 
            msg['Bcc'] = ", ".join(recipients)  # BCCë¡œ ì„¤ì •
            recipient_display = f"{len(recipients)}ëª… (BCC)"
        else:
            msg['To'] = ""
            msg['Bcc'] = recipients
            recipient_display = "1ëª… (BCC)"
            recipients = [recipients]  # ë‹¨ì¼ ìˆ˜ì‹ ìë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜ - News run_19Jun25 í˜•ì‹
        msg['Subject'] = f"News run_{mail_date_str}"
        
        # HTML í˜•ì‹ìœ¼ë¡œ ì´ë©”ì¼ ë³¸ë¬¸ ìƒì„± (Boldì™€ Underlineì„ ìœ„í•´)
        body_html = """<html>
<head>
    <style>
        body { font-family: Arial, sans-serif; font-size: 14px; }
        .group-header { font-weight: bold; text-decoration: underline; margin-top: 15px; margin-bottom: 5px; }
        .article-item { margin-left: 20px; margin-bottom: 3px; }
        .signature { margin-top: 20px; }
    </style>
</head>
<body>
    <p>Dear All,</p>
    <p>Please see attached today's news.</p>
"""
        
        # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ ëª©ë¡ ì¶”ê°€ (ì •ì˜ëœ ìˆœì„œëŒ€ë¡œ)
        for group in GROUP_ORDER:
            if group in grouped_news and grouped_news[group]:
                body_html += f'<div class="group-header">{group}</div>\n'
                for idx, article in enumerate(grouped_news[group], 1):
                    # ì œëª© ì²˜ë¦¬ (ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°)
                    title = article['title']
                    if len(title) > 50:
                        title = title[:50] + '...'
                    body_html += f'<div class="article-item">{idx}. {title} [{article["date"]}. {article["source"]}]</div>\n'
        
        # ê·¸ë£¹ì— ì†í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš° Othersë¡œ í‘œì‹œ
        other_keywords = []
        for idx, row in df.iterrows():
            keyword = row.get('ê²€ìƒ‰í‚¤ì›Œë“œ', row.get('ê²€ìƒ‰ í‚¤ì›Œë“œ', row.get('í‚¤ì›Œë“œ', '')))
            if pd.isna(keyword) or keyword == '':
                continue
            if keyword not in KEYWORD_GROUPING:
                other_keywords.append(keyword)
        
        if other_keywords:
            unique_others = list(set(other_keywords))
            if unique_others:
                body_html += '<div class="group-header">Others</div>\n'
                body_html += f'<div class="article-item">Keywords not grouped: {", ".join(unique_others)}</div>\n'
        
        body_html += """
    <div class="signature">
        <p>Thank you.</p>
        <p>Best regards,<br>
        Sangmin Park</p>
    </div>
</body>
</html>
"""
        
        # HTML ë©”ì‹œì§€ ì²¨ë¶€
        msg.attach(MIMEText(body_html, 'html'))
        
        # PDF íŒŒì¼ ì²¨ë¶€
        try:
            with open(pdf_file, "rb") as f:
                part = MIMEBase('application', 'octet-stream')
                part.set_payload(f.read())
            encoders.encode_base64(part)
            
            english_filename = get_english_filename(pdf_file, today_str)
            print(f"   ğŸ“ PDF ì²¨ë¶€: {english_filename}")
            
            part.add_header(
                'Content-Disposition',
                f'attachment; filename="{english_filename}"'
            )
            msg.attach(part)
            
            print(f"   âœ… PDF íŒŒì¼ ì²¨ë¶€ ì™„ë£Œ: {english_filename} ({pdf_size_mb:.1f}MB)")
        except Exception as e:
            print(f"   âŒ PDF íŒŒì¼ ì²¨ë¶€ ì‹¤íŒ¨: {e}")
            return False
        
        # SMTP ì„œë²„ ì—°ê²° ë° ì´ë©”ì¼ ë°œì†¡
        print("   ğŸ”„ SMTP ì„œë²„ì— ì—°ê²° ì¤‘...")
        server = smtplib.SMTP(EMAIL_CONFIG["smtp_server"], EMAIL_CONFIG["smtp_port"])
        server.starttls()
        print("   ğŸ” ë¡œê·¸ì¸ ì¤‘...")
        server.login(EMAIL_CONFIG["sender_email"], EMAIL_CONFIG["sender_password"])
        print("   ğŸ“¤ ì´ë©”ì¼ ë°œì†¡ ì¤‘...")
        
        # sendmail ì‚¬ìš© (send_message ëŒ€ì‹ )
        server.sendmail(EMAIL_CONFIG["sender_email"], recipients, msg.as_string())
            
        server.quit()
        
        print(f"\n   ğŸ‰ ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ!")
        print(f"   ğŸ“¬ ìˆ˜ì‹ ì: {recipient_display}")
        print(f"   ğŸ“… ë°œì†¡ ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}")
        return True
        
    except Exception as e:
        print(f"\n   âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
        print("   ğŸ’¡ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ ì£¼ì„¸ìš”:")
        print("      - ì´ë©”ì¼ ì£¼ì†Œì™€ ë¹„ë°€ë²ˆí˜¸")
        print("      - ì¸í„°ë„· ì—°ê²° ìƒíƒœ")
        print("      - Gmail 2ë‹¨ê³„ ì¸ì¦ ë° ì•± ë¹„ë°€ë²ˆí˜¸")
        print("      - PDF íŒŒì¼ í¬ê¸° (25MB ì œí•œ)")
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    driver = None
    
    try:
        print("=" * 70)
        print("ğŸ”” ë‰´ìŠ¤ í†µí•© ìŠ¤í¬ë˜í•‘ ë° PDF ìƒì„± ì‹œì‘")
        print("=" * 70)
        print(f"ğŸ“… ë‚ ì§œ: {datetime.today().strftime('%Yë…„ %mì›” %dì¼')}")
        
        # ìˆ˜ì‹ ì ì •ë³´ ì¶œë ¥
        recipients = EMAIL_CONFIG['recipient_email']
        if isinstance(recipients, list):
            print(f"ğŸ“§ ìˆ˜ì‹ ì: {len(recipients)}ëª…")
            for i, email in enumerate(recipients, 1):
                print(f"   {i}. {email}")
        else:
            print(f"ğŸ“§ ìˆ˜ì‹ ì: {recipients}")
            
        print(f"ğŸ“Š ëŒ€ìƒ íŒŒì¼: {excel_filename}")
        print()
        
        # ì—‘ì…€ íŒŒì¼ í™•ì¸
        print(f"ğŸ“Š ì—‘ì…€ íŒŒì¼ í™•ì¸...")
        if not os.path.exists(excel_filename):
            print(f"   âŒ ì—‘ì…€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {excel_filename}")
            return
        print(f"   âœ… ì—‘ì…€ íŒŒì¼ ë°œê²¬: {excel_filename}")
        
        # Chrome ì„¤ì • ë° ì‹œì‘
        print("\nğŸŒ Chrome ë¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        options = Options()
        options.binary_location = chrome_path
        options.add_argument("--headless=new")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--window-size=1920,1080")
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service)
        print("   âœ… ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ")
        
        # í†µí•© PDF ìƒì„±
        pdf_file, success_count, site_stats = generate_unified_pdf(driver)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print(f"\n{'='*50}")
        print("ğŸ“‹ ì „ì²´ ì‘ì—… ê²°ê³¼ ìš”ì•½")
        print(f"{'='*50}")
        
        total_processed = sum(stats['total'] for stats in site_stats.values())
        total_success = sum(stats['success'] for stats in site_stats.values())
        total_failed = sum(stats['failed'] for stats in site_stats.values())
        
        print(f"ğŸ“Š ì „ì²´ í†µê³„:")
        print(f"   - ì´ ê¸°ì‚¬ ìˆ˜: {total_processed}ê°œ")
        print(f"   - ì„±ê³µ: {total_success}ê°œ")
        print(f"   - ì‹¤íŒ¨: {total_failed}ê°œ")
        print(f"   - ì„±ê³µë¥ : {(total_success/total_processed*100):.1f}%" if total_processed > 0 else "   - ì„±ê³µë¥ : 0%")
        
        print(f"\nğŸ“ˆ ì‚¬ì´íŠ¸ë³„ ìƒì„¸:")
        for site_key, stats in site_stats.items():
            success_rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
            print(f"   - {stats['name']}: {stats['success']}/{stats['total']}ê°œ ({success_rate:.1f}%)")
        
        if pdf_file:
            pdf_size_mb = os.path.getsize(pdf_file) / (1024 * 1024)
            print(f"\nğŸ“„ í†µí•© PDF: {pdf_file} ({pdf_size_mb:.1f}MB)")
        
        # ì´ë©”ì¼ ë°œì†¡
        if total_success > 0:
            email_success = send_email_with_pdf(pdf_file, total_processed, site_stats)
            
            if email_success:
                print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                print("   âœ… ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ")
            else:
                print("\nâš ï¸ PDF ìƒì„±ì€ ì™„ë£Œë˜ì—ˆì§€ë§Œ ì´ë©”ì¼ ë°œì†¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâ— ì„±ê³µí•œ ì‘ì—…ì´ ì—†ì–´ ì´ë©”ì¼ì„ ë°œì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"\nâŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    finally:
        if driver:
            driver.quit()
            print("\nğŸšª ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        
        print("\n" + "=" * 70)
        print("ì‘ì—… ì™„ë£Œ")
        print("=" * 70)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    main()